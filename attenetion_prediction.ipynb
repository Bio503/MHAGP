{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import copy\n",
    "from util import normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train_gd = normalization(np.load('./data/feature/X_train_gd_nl.npy'))\n",
    "X_train_gmd = normalization(np.load('./data/feature/X_train_gmd_nl.npy'))\n",
    "X_train_gld = normalization(np.load('./data/feature/X_train_gld_nl.npy')) \n",
    "\n",
    "X_test_gd = normalization(np.load('./data/feature/X_test_gd_nl.npy'))\n",
    "X_test_gmd = normalization(np.load('./data/feature/X_test_gmd_nl.npy'))\n",
    "X_test_gld = normalization(np.load('./data/feature/X_test_gld_nl.npy'))\n",
    "\n",
    "X_gd = np.concatenate((X_train_gd,X_test_gd))\n",
    "X_gmd = np.concatenate((X_train_gmd,X_test_gmd))\n",
    "X_gld = np.concatenate((X_train_gld,X_test_gld))\n",
    "print(X_gd.shape,X_gmd.shape,X_gld.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=open('./data/y_train.txt').read().split('\\n')[:-1] \n",
    "y_train=[int(y) for y in y_train]\n",
    "y_train=np.array(y_train)\n",
    "y_train=y_train.reshape(y_train.shape[0],1)\n",
    "\n",
    "y_test=open('./data/y_test.txt').read().split('\\n')[:-1]   \n",
    "y_test=[int(y) for y in y_test]\n",
    "y_test = np.array(y_test)\n",
    "y_test =y_test.reshape(y_test.shape[0],1)\n",
    "\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12864\\1946973125.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_gd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_gd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_gd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_gmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_gmd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_gmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_gld\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_gld\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_gld\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_gd_all\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_gd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "X_gd = np.concatenate((X_train_gd,X_test_gd))\n",
    "X_gmd = np.concatenate((X_train_gmd,X_test_gmd))\n",
    "X_gld = np.concatenate((X_train_gld,X_test_gld))\n",
    "\n",
    "X_gd_all =np.copy(X_gd) \n",
    "X_gd_all =normalization(X_gd_all)\n",
    "X_gd_all = torch.from_numpy(X_gd_all).type(torch.FloatTensor)\n",
    "\n",
    "X_gmd_all =np.copy(X_gmd)   \n",
    "X_gmd_all =normalization(X_gmd_all)\n",
    "X_gmd_all = torch.from_numpy(X_gmd_all).type(torch.FloatTensor)\n",
    "\n",
    "X_gld_all =np.copy(X_gld)  \n",
    "X_gld_all =normalization(X_gld_all)\n",
    "X_gld_all = torch.from_numpy(X_gld_all).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "y_all =np.concatenate((y_train,y_test))   \n",
    "y_all =torch.from_numpy(y_all).type(torch.FloatTensor)\n",
    "\n",
    "X_gd_all.size(),X_gmd_all.size(),X_gld_all.size(),y_all.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGAClassifier(nn.Module):\n",
    "#     def __init__(self,inp_size):\n",
    "    def __init__(self,inp_size,inp_size_bak1,inp_size_bak2):\n",
    "        super(DGAClassifier,self).__init__()\n",
    "        #         self.att = nn.Linear(inp_size,inp_size)\n",
    "        self.fc1 = nn.Linear(inp_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, 1)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.multihead_attn = torch.nn.MultiheadAttention(embed_dim = inp_size, num_heads =8 )\n",
    "        self.linear = nn.Linear(256,128)\n",
    "        self.linear256 = nn.Linear(256,256)\n",
    "        self.linear384 = nn.Linear(384,256)\n",
    "        self.linear512 = nn.Linear(512,256)\n",
    "    def forward(self, input_,input_gmd,input_gld):\n",
    "#         print(\"__init__:input_\",input_)\n",
    "#         t = self.att(input_)\n",
    "#         s = self.tanh(t)\n",
    "        # GD+GMD+GLD\n",
    "        gd_md,attn_output_weights = self.multihead_attn(input_gmd,input_,input_)\n",
    "        gd_ld,attn_output_weights = self.multihead_attn(input_gld,input_,input_)\n",
    "        gd_mld = torch.cat((gd_md,gd_ld,input_),1)   # 768\n",
    "        gd_mld = self.linear384(gd_mld) \n",
    "        input_ = gd_mld\n",
    "        # GD+GMD\n",
    "        # input_gd_gmd,attn_output_weights = self.multihead_attn(input_gmd,input_,input_)  \n",
    "        # input_ = torch.cat((input_gd_gmd,input_),1)\n",
    "        # input_ = self.linear512(input_)\n",
    "\n",
    "        # GD+GLD\n",
    "#         input_gd_gld,attn_output_weights = self.multihead_attn(input_gld,input_,input_)  \n",
    "#         input_ = torch.cat((input_gd_gld,input_),1)\n",
    "#         input_ = self.linear384(input_)       \n",
    "        # GD\n",
    "#         input_,attn_output_weights = self.multihead_attn(input_,input_,input_)  \n",
    "#         input_ = torch.cat((input_,input_),1)\n",
    "#         input_ = self.linear256(input_)\n",
    "        \n",
    "        s = self.tanh(input_)       #s= torch.Size([batch_size, 128])  alpha= torch.Size([batch_size, 128])\n",
    "        alpha = self.softmax(s)\n",
    "        x = torch.mul(alpha,input_)\n",
    "        a1 = self.fc1(x)\n",
    "        h1 = self.relu(a1)\n",
    "        dout = self.dout(h1)   # 损失率\n",
    "        a2 = self.fc2(dout)\n",
    "        h2 = self.relu(a2)\n",
    "        a3 = self.out(h2)\n",
    "        y = self.out_act(a3)\n",
    "        return y\n",
    "    \n",
    "    def predict(self,x,x_gmd,x_gld): \n",
    "        #Apply softmax to output. \n",
    "        pred = self.forward(x,x_gmd,x_gld)\n",
    "        ans = []\n",
    "        #Pick the class with maximum weight\n",
    "        for t in pred:\n",
    "            if t<0.5:\n",
    "                ans.append(0)\n",
    "            else:\n",
    "                ans.append(1)\n",
    "        return torch.tensor(ans)\n",
    "    \n",
    "    def predict_probability(self,x,x_gmd,x_gld):\n",
    "        pred = self.forward(x，x_gmd,x_gld)\n",
    "        ans = pred\n",
    "        return torch.tensor(ans)\n",
    "    \n",
    "    def predict_probability2(self,x,x_gmd,x_gld):\n",
    "        pred = self.forward(x，x_gmd,x_gld)\n",
    "        pred_score = pred\n",
    "        ans = []\n",
    "        for t in pred:\n",
    "            if t<0.5:\n",
    "                ans.append(0)\n",
    "            else:\n",
    "                ans.append(1)\n",
    "        return torch.tensor(pred_score),torch.tensor(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv(model, X, X_gmd, X_gld, y, opt, criterion, batch_size=64): \n",
    "    model.train()\n",
    "    losses = []\n",
    "    for beg_i in range(0, X.size(0), batch_size):\n",
    "        x_batch = X[beg_i:beg_i + batch_size, :]\n",
    "        x_gmd_batch = X_gmd[beg_i:beg_i + batch_size, :]\n",
    "        x_gld_batch = X_gld[beg_i:beg_i + batch_size, :]\n",
    "        \n",
    "        y_batch = y[beg_i:beg_i + batch_size, :]\n",
    "        x_batch = Variable(x_batch)\n",
    "        x_gmd_batch = Variable(x_gmd_batch)\n",
    "        x_gld_batch = Variable(x_gld_batch)\n",
    "        y_batch = Variable(y_batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        # (1) Forward\n",
    "        y_hat = clf(x_batch, x_gmd_batch, x_gld_batch)\n",
    "        # (2) Compute diff\n",
    "        loss = criterion(y_hat, y_batch)\n",
    "        # (3) Compute gradients\n",
    "        loss.backward()\n",
    "        # (4) update weights\n",
    "        opt.step()        \n",
    "        losses.append(loss.data.numpy())\n",
    "    avg_loss=sum(losses)/len(losses)\n",
    "    return losses,avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(l):\n",
    "    return sum(l)/len(l)\n",
    "def compute_aupr(y,yhat):\n",
    "    pre,rec,_ = precision_recall_curve(y,yhat)\n",
    "    return auc(rec,pre)\n",
    "def f1_score(precison, recall):\n",
    "    try:\n",
    "        return 2*precison*recall /(precison + recall)\n",
    "    except:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fpr_list=[]\n",
    "tpr_list=[]\n",
    "aucs = []\n",
    "avg_pre = []\n",
    "aupr = []\n",
    "acc = []\n",
    "recal = []\n",
    "cv = StratifiedKFold(n_splits=5)    \n",
    "num_epochs = 30\n",
    "fold_loss =[]\n",
    "gmd_list = []\n",
    "for i, (train,test) in enumerate(cv.split(X_gmd_all, y_all)):\n",
    "    gmd_list.append((train,test))\n",
    "gld_list = []\n",
    "for i, (train,test) in enumerate(cv.split(X_gld_all, y_all)):\n",
    "    gld_list.append((train,test))\n",
    "\n",
    "print(\"start\",'-'*100)\n",
    "for i, (train,test) in enumerate(cv.split(X_gd_all, y_all)):\n",
    "\n",
    "    clf = GDAClassifier(X_gd_all.size()[1],X_gmd_all.size()[1],X_gld_all.size()[1]) \n",
    "    opt = optim.Adam(clf.parameters(), lr=0.01, betas=(0.9, 0.999),weight_decay=1e-5)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    e_losses = []\n",
    "    for e in range(num_epochs):\n",
    "        los,avg_loss = train_cv(clf,X_gd_all[train],X_gmd_all[train],X_gld_all[train],y_all[train],opt,criterion)   \n",
    "        e_losses += [avg_loss]\n",
    "        print (\"Fold:%d Epoch:%d/%d Train loss:%f\" %(i+1,e,num_epochs,avg_loss))\n",
    "        \n",
    "    fold_loss.append(e_losses)   \n",
    "    # y_pred = clf.predict(X_gd_all[test], X_gmd_all[test], X_gld_all[test])\n",
    "    y_pred_roc,y_pred = clf.predict_probability2(X_gd_all[test], X_gmd_all[test], X_gld_all[test])\n",
    "    fpr,tpr,_= roc_curve(y_all[test],y_pred_roc)\n",
    "    fpr_list.append(fpr)\n",
    "    tpr_list.append(tpr)\n",
    "    aucs.append(roc_auc_score(y_all[test],y_pred))\n",
    "    recal.append(recall_score(y_all[test],y_pred))\n",
    "    print(\"Test accuracy:%f\" %acc[-1])\n",
    "    print('\\n')\n",
    "\n",
    "avr_f1_score = f1_score(avg(avg_pre), avg(recal))\n",
    "\n",
    "print (\"AUC:%f AUPR: %f Avg Pre: %f Acc: %f _f1_score: %f Recal:%f\" %(avg(aucs),avg(aupr),avg(avg_pre),avg(acc),avr_f1_score,avg(recal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(aucs)):\n",
    "    plt.plot(fpr_list[i],tpr_list[i],label=\"Fold-\"+str(i+1)+\"(auc=\"+str(round(aucs[i],3))+\")\")\n",
    "    plt.plot([0,1], [0,1], color='navy', lw=1, linestyle='--')\n",
    "    plt.legend()\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.xlim([0, 1.0])\n",
    "plt.ylim([0, 1.05])\n",
    "plt.title('ROC curves of GMHAGP')\n",
    "plt.savefig('./data/nl_ROC.svg',dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearn] *",
   "language": "python",
   "name": "conda-env-deeplearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
